## LLM

[本地大模型之路（一）：大模型的是什么、为什么以及怎么选](https://sspai.com/post/94875)

[本地大模型之路（二）：了解模型能力与性能需求，让硬件选购恰到好处](https://sspai.com/post/95262)

[本地大模型之路（三）：推理引擎和 LLM 应用](https://sspai.com/post/96911)

[最好的致敬是学习：DeepSeek-R1 赏析](https://mp.weixin.qq.com/s/_XGBipbywCOtcKu13QDW5Q)

[MCP 协议：LLM 应用开发的 “Type-C”](https://my.oschina.net/u/4489239/blog/17989059)

## 入门
[CPU vs. GPU](https://www.infoq.cn/article/UU1l5ayRqQBNyKZtLFQo?utm_source=rss&utm_medium=article)

[机器学习知识体系](https://www.zhihu.com/question/266291909/answer/2543083234)

[沈向洋：浅谈人工智能创造](https://event.baai.ac.cn/play/89)  [.pdf](data/static_pages/pdf/1.pdf) 

## 理解

[Model Distillation Minimal Example](https://my.oschina.net/IDP/blog/18441778)

[TensorFlow 篇 | TensorFlow 2.x 模型 Serving 服务](https://flashgene.com/archives/154963.html)

[Keras Optimizers](https://keras.io/zh/optimizers/#adagrad)

[Keras activations](https://keras.io/zh/activations/#sigmoid)

[SHAP：Python的可解释机器学习库](https://zhuanlan.zhihu.com/p/83412330)

[可解释性机器学习_Feature Importance、Permutation Importance、SHAP](https://blog.csdn.net/weixin_44803791/article/details/109776357)

[深入学习卷积神经网络中卷积层和池化层的意义](https://www.cnblogs.com/wj-1314/p/9593364.html)

[一文搞懂RNN（循环神经网络）基础篇](https://zhuanlan.zhihu.com/p/30844905)
>循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。

[过拟合](https://juejin.cn/post/7087483936237944839)
>Dropout 可以认为是一种模型训练方法，其中每批数据训练时只有一定比例的权重被更新，而其余的权重不被更新

[最通俗易懂的注意力机制（Attention）讲解](https://www.bilibili.com/video/BV1rNoqYWEbd?spm_id_from=333.788.videopod.sections&vd_source=8a2ac354cbb86c18cb6850120d9cf644)

## 应用

[让你彻底弄懂 MCP 的一切](https://www.escapelife.site/posts/3ac10bcf.html)

[GGUF](https://my.oschina.net/HuggingFace/blog/18017572)

[AI让效率再次提升，人类该干什么？](https://tumutanzi.com/archives/17493)

[Interactive Video Stylization Using Few-Shot Patch-Based Training](https://github.com/OndrejTexler/Few-Shot-Patch-Based-Training)

[AI 作曲 （完整思路）](https://eurychen.me/post/music/ai-compose-music/)
>美不能被量化，但却有迹可循。AI 三脚猫功夫般的作曲，即是一种证明。
